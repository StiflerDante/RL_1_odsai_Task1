# -*- coding: utf-8 -*-
"""ДЗ_RL_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XsYMw0HvQ6-BPNbfUnBvuy81QAqRtWR0
"""

!apt-get install x11-utils > /dev/null 2>&1
!pip install pyglet > /dev/null 2>&1
!apt-get install -y xvfb python-opengl > /dev/null 2>&1 # Без этого не запускались картинки
!pip install gym

import gym
import numpy as np
import time
import random
import matplotlib.pyplot as plt

env = gym.make("Taxi-v3").env #Создаем окружение(структуру окружения) для игры Taxi

env.seed(0) # Запоминаем окружение
env.reset() #Создаем количество состояний

iteration_n = 100
trajectory_n = 100 # Еще один параметр для оптимизации алгоритма. Количество траекторий
q_param = 0.9

for i in range(interation_n):
  #Policy evaluation
  trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)] # Получаем массив траекторий
  total_rewards = [np.sum(trajectory["rewards"]) for trajectory in trajectories]
  print("Iteration:", i, "Mean total reward:", np.mean(total_rewards)) # По мере обучения этот показатель должен расти

  #policy improvement. Здесь задача выбрать элитный траектории. Мы их выбираем опираясь на квантиль. У нас есть общие награды (total_rewards) и есть некий параметр q и нам нужно получить квантиль
  quantile = np.quantile(total_rewards, q_param) #Гамма q
  # Теперь нужно из всех траекторий оставить только элитные. Элитные - те, у которых total_reward выше гамма q
  elite_trajectories = list()

  for trajectory in trajectories:
    total_reward = np.sum(trajectory["rewards"])
    if total_reward > quantile:
      elite_trajectories.append(trajectory)



  agent.fit(elite_trajectories)

class CrossEntropyAgent():
  def __init__(self, state_n, action_n): # Движения агенты определяются матрицей у которой строки являются состоянием, а столбцы - действия

    self.state_n = state_n

    self.action_n = action_n

    self.model = np.ones((self.state_n, self.action_n)) / action_n #инициализируем матрицу т.к. внутренняя модель будет определяться матрицей. При этом мы хотим чтобы эта матрица вначале соответововала равномерной политике для всех состояний. В данном случае у нас 6 действий поэтому 6 столбцов, а сумма по строке должна быть равна 1

  def get_action(self, state):
    action = np.random.choice(np.arange(self.action_n), p = self.model[state]) #Вместо жадного алгоритма(максимума), берем рандомный выбор вероятности движения

    return int(action)

  def fit(self, elite_trajectories): #Здесь и проходит обучение - под обучением подразумевается изменение модели (self.model). Политика определяется по набору траекторий
    new_model = np.zeroes((self.state_n, self.action_n))

    for trajectory in elite_trajectories:
      for state,action in zip(trajectory["states"], trajectory["actions"]):
        new_model[state][action] += 1


      for state in states:
        if np.sum(new_model[state]) > 0:
          new_model[state] /= np.sum(new_model[state])

        else:
          new_model[state] = self.model[state].copy()

      self.model = new_model

      return None

class RandomAgent():
  def __init__(self, action_n):
    self.action_n = action_n


  def get_action(self, state):
    action = np.random.randint(self.action_n)
    return action


class CrossEntropyAgent():
  def __init__(self, state_n, action_n): # Движения агенты определяются матрицей у которой строки являются состоянием, а столбцы - действия

    self.state_n = state_n

    self.action_n = action_n

    self.model = np.ones((self.state_n, self.action_n)) / self.action_n #инициализируем матрицу т.к. внутренняя модель будет определяться матрицей. При этом мы хотим чтобы эта матрица вначале соответововала равномерной политике для всех состояний. В данном случае у нас 6 действий поэтому 6 столбцов, а сумма по строке должна быть равна 1

  def get_action(self, state):
    action = np.random.choice(np.arange(self.action_n), p = self.model[state]) #Вместо жадного алгоритма(максимума), берем рандомный выбор вероятности движения

    return int(action)

  def fit(self, elite_trajectories): #Здесь и проходит обучение - под обучением подразумевается изменение модели (self.model). Политика определяется по набору траекторий
    new_model = np.zeroes((self.state_n, self.action_n))

    for trajectory in elite_trajectories:
      for state,action in zip(trajectory["states"], trajectory["actions"]):
        new_model[state][action] += 1


    for state in range(self.state_n):
      if np.sum(new_model[state]) > 0:
        new_model[state] /= np.sum(new_model[state])

      else:
        new_model[state] = self.model[state].copy()

    self.model = new_model
    return None

def get_state(obs):
  return int(obs)

def get_trajectory(env, agent, max_len = 1000, visualize = True):
  trajectory = {"states": [],
                "actions": [],
                "rewards": []}
  obs = env.reset()
  state = get_state(obs)

  for i in range(max_len):
    trajectory["states"].append(state)

    action = agent.get_action(state)
    trajectory["actions"].append(action)

    obs, reward, done, _ = env.step(action)
    trajectory["rewards"].append(reward)

    state = get_state(obs)

    if visualize:
      time.sleep(0.01)
      a = env.render(mode = 'rgb_array')
      plt.imshow(a)


    if done:
      break

  return trajectory

agent = CrossEntropyAgent(state_n, action_n)
q_param = 0.9
iteration_n = 20
trajectory_n = 50

for i in range(iteration_n):
  #Policy evaluation
  trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)] # Получаем массив траекторий
  total_rewards = [np.sum(trajectory["rewards"]) for trajectory in trajectories]
  print("Iteration:", i, "Mean total reward:", np.mean(total_rewards)) # По мере обучения этот показатель должен расти

  #policy improvement. Здесь задача выбрать элитный траектории. Мы их выбираем опираясь на квантиль. У нас есть общие награды (total_rewards) и есть некий параметр q и нам нужно получить квантиль
  quantile = np.quantile(total_rewards, q_param) #Гамма q
  # Теперь нужно из всех траекторий оставить только элитные. Элитные - те, у которых total_reward выше гамма q
  elite_trajectories = list()

  for trajectory in trajectories:
    total_reward = np.sum(trajectory["rewards"])
    if total_reward > quantile:
      elite_trajectories.append(trajectory)



  agent.fit(elite_trajectories)

trajectory = get_trajectory(env, agent, max_len=100, visualize=True)
print('total reward:', sum(trajectory['rewards']))
print('model:')
print(agent.model)

#Создаем более оптимальный вариант + grid search

env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6


class RandomAgent():
    def __init__(self, action_n):
        self.action_n = action_n

    def get_action(self, state):
        action = np.random.randint(self.action_n)
        return action


class CrossEntropyAgent():
    def __init__(self, state_n, action_n):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((self.state_n, self.action_n)) / self.action_n

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        self.model = new_model
        return None


def get_state(obs):
    return int(obs)


def get_trajectory(env, agent, max_len=1000, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            time.sleep(0.5)
            env.render()

        if done:
            break

    return trajectory


agent = CrossEntropyAgent(state_n, action_n)
q_param = 0.9
iteration_n = 20
trajectory_n = 50

for iteration in range(iteration_n):

    #policy evaluation
    trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
    total_rewards = [np.sum(trajectory['rewards']) for trajectory in trajectories]
    print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))

    #policy improvement
    quantile = np.quantile(total_rewards, q_param)
    elite_trajectories = []
    for trajectory in trajectories:
        total_reward = np.sum(trajectory['rewards'])
        if total_reward > quantile:
            elite_trajectories.append(trajectory)

    agent.fit(elite_trajectories)

trajectory = get_trajectory(env, agent, max_len=100, visualize=True)
print('total reward:', sum(trajectory['rewards']))
print('model:')
print(agent.model)

"""# **Задание 1**"""

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.1],
    "iteration_n": [500],
    "trajectory_n": [100]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.4],
    "iteration_n": [500],
    "trajectory_n": [100]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards, q_param = 0.4, Trajectory_n = 100", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.4],
    "iteration_n": [500],
    "trajectory_n": [200]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.9],
    "iteration_n": [500],
    "trajectory_n": [100]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [200]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [300]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}

env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
# grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory

hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [400]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

"""# **Задание 2**"""

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories, smooth_lambda = 0.01):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()
        new_model = (new_model + smooth_lambda) / (np.sum(new_model, axis=1)[:, np.newaxis] + (self.action_n * smooth_lambda))

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)
        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [300]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories, smooth_lambda = 0.5):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()
        new_model = (new_model + smooth_lambda) / (np.sum(new_model, axis=1)[:, np.newaxis] + (self.action_n * smooth_lambda))

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [300]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(296), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None, t=1.0):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model
        self.t = t

    def get_action(self, state):
        #policy smoothing
        logits = self.model[state] / self.t
        softmax_probs = np.exp(logits) / np.sum(np.exp(logits))
        action = np.random.choice(np.arange(self.action_n), p=softmax_probs)
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory


hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [300]
}

# Initialize the best results
best_mean_total_reward = -float("inf")
best_hyperparameters = {}


env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(77), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None, t=0.1):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model
        self.t = t

    def get_action(self, state):
        #policy smoothing
        logits = self.model[state] / self.t
        softmax_probs = np.exp(logits) / np.sum(np.exp(logits))
        action = np.random.choice(np.arange(self.action_n), p=softmax_probs)
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory

hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [300]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}

env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
# grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(151), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None, t=0.5):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model
        self.t = t

    def get_action(self, state):
        #policy smoothing
        logits = self.model[state] / self.t
        softmax_probs = np.exp(logits) / np.sum(np.exp(logits))
        action = np.random.choice(np.arange(self.action_n), p=softmax_probs)
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    obs = env.reset()
    state = get_state(obs)

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        obs, reward, done, _ = env.step(action)
        trajectory['rewards'].append(reward)

        state = get_state(obs)

        if visualize:
            env.render()

    return trajectory

hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [300]
}

best_mean_total_reward = -float("inf")
best_hyperparameters = {}

#Taxi environment
env = gym.make("Taxi-v3").env
state_n = 500
action_n = 6
rew = list()
# grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(51), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()

"""# **Задание 3**"""

class CrossEntropyAgent():
    def __init__(self, state_n, action_n, model=None):
        self.state_n = state_n
        self.action_n = action_n
        self.model = np.ones((state_n, action_n)) / action_n if model is None else model

    def get_action(self, state):
        action = np.random.choice(np.arange(self.action_n), p=self.model[state])
        return int(action)

    def fit(self, elite_trajectories):
        new_model = np.zeros((self.state_n, self.action_n))
        for trajectory in elite_trajectories:
            for state, action in zip(trajectory['states'], trajectory['actions']):
                new_model[state][action] += 1

        for state in range(self.state_n):
            if np.sum(new_model[state]) > 0:
                new_model[state] /= np.sum(new_model[state])
            else:
                new_model[state] = self.model[state].copy()

        return CrossEntropyAgent(self.state_n, self.action_n, new_model)

def get_state(obs):
    return int(obs)

def get_trajectory(env, agent, max_len=200, visualize=False):
    trajectory = {'states': [], 'actions': [], 'rewards': []}

    state = get_state(env.reset())

    for _ in range(max_len):
        trajectory['states'].append(state)

        action = agent.get_action(state)
        obs, reward, done, _ = env.step(action)
        trajectory['actions'].append(action)
        trajectory['rewards'].append(reward)
        state = get_state(obs)

        if visualize:
            env.render()

        if done:
            break

    return trajectory


hyperparameters = {
    "q_param": [0.6],
    "iteration_n": [500],
    "trajectory_n": [300]
}


best_mean_total_reward = -float("inf")
best_hyperparameters = {}

env = gym.make("Taxi-v3").env
state_n = env.observation_space.n
action_n = env.action_space.n
rew = list()

#grid search
for q_param in hyperparameters["q_param"]:
    for iteration_n in hyperparameters["iteration_n"]:
        for trajectory_n in hyperparameters["trajectory_n"]:
            agent = CrossEntropyAgent(state_n, action_n)
            total_rewards = []

            for iteration in range(iteration_n):
                trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]
                total_rewards.extend([np.sum(trajectory['rewards']) for trajectory in trajectories])
                print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))
                rew.append(np.mean(total_rewards))

                quantile = np.quantile(total_rewards, q_param)
                elite_trajectories = [trajectory for trajectory in trajectories if np.sum(trajectory['rewards']) > quantile]
                agent = agent.fit(elite_trajectories)

            mean_total_reward = np.mean(total_rewards)
            if mean_total_reward > best_mean_total_reward:
                best_mean_total_reward = mean_total_reward
                best_hyperparameters = {"q_param": q_param, "iteration_n": iteration_n, "trajectory_n": trajectory_n}

print("Best Hyperparameters:", best_hyperparameters)
print("Best Mean Total Reward:", best_mean_total_reward)

plt.plot(range(500), rew, linestyle = 'dashed', color = 'r', linewidth = '2')


font1 = {'family':'serif','color':'blue','size':16}
font2 = {'family':'serif','color':'darkred','size':12}

plt.title("График Rewards", fontdict = font1)
plt.xlabel("Iterations", fontdict = font2)
plt.ylabel("Mean reward", fontdict = font2)

plt.grid()

plt.show()